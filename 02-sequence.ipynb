{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9e8209",
   "metadata": {},
   "source": [
    "# Sequence Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d81b2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars =  list(\"abcdefghijklmnopqrstuvwxyz -_,\")\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for c, i in ctoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a983c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3860ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hello world, this is bread modular'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello world, this is Bread Modular\".lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96c9dd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  4, 11, 11, 14, 26, 22, 14, 17, 11,  3, 29, 26, 19,  7,  8, 18, 26,\n",
       "         8, 18, 26,  1, 17,  4,  0,  3, 26, 12, 14,  3, 20, 11,  0, 17])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the text into a pytorch list\n",
    "data = torch.tensor([ctoi[c] for c in text], dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e07ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM usually work with blocks. Here use 8\n",
    "## GPT3 seems to be use 2048\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d03bb260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8, 18, 26,  8, 18, 26,  1, 17]),\n",
       " tensor([18, 26,  8, 18, 26,  1, 17,  4]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randint(len(data) - block_size - 1, (1,))\n",
    "x = data[i: i+block_size]\n",
    "y = data[i + 1: i + 1 + block_size]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f85c8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(d):\n",
    "    i = torch.randint(len(d) - block_size - 1, (1,))\n",
    "    x = data[i : i + block_size]\n",
    "    y = data[i + 1 : i + 1 + block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afdf4fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([26,  8, 18, 26,  1, 17,  4,  0]),\n",
       " tensor([ 8, 18, 26,  1, 17,  4,  0,  3]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d0d8a",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Here we trying to get training data. The whole idea of a LLM is to predit the next word. In our case, we are trying to predict the next letter.\n",
    "\n",
    "So, this is we are building the training dataset for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82840635",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
