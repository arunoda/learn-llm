{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a0b8c5",
   "metadata": {},
   "source": [
    "# Bigram Model\n",
    "\n",
    "Here we are trying to make a model which predict the next letter given a set of characters. \n",
    "\n",
    "We train very little data and compute but still, it's giving us some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09434900",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars =  list(\"abcdefghijklmnopqrstuvwxyz -_,\")\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for c, i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99d5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    return [ctoi[c] for c in text.lower()]\n",
    "\n",
    "def decode_text(encoded_text):\n",
    "    return \"\".join([itoc[i] for i in encoded_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06e7d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  4, 11, 11, 14, 26, 22, 14, 17, 11,  3, 29, 26, 19,  7,  8, 18, 26,\n",
       "         8, 18, 26,  1, 17,  4,  0,  3, 26, 12, 14,  3, 20, 11,  0, 17])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = len(chars)\n",
    "block_size = 8\n",
    "text = \"hello world, this is Bread Modular\"\n",
    "data = torch.tensor(encode_text(text), dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d97e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(d):\n",
    "    i = torch.randint(len(d) - block_size - 1, (1,))\n",
    "    x = data[i : i + block_size]\n",
    "    y = data[i + 1 : i + 1 + block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e29340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # first is the number of embeddings, second is the embedding dimension\n",
    "        # here we use cross_entropy loss, so we need the dimension to be the same as no of embeddings\n",
    "        # These are the weights of this model, so it trains how likely the next letter for a given letter\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # logits are just numbers shows the weights or predictions \n",
    "        logits = self.token_embedding(idx)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0292023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, torch.Size([5]), torch.Size([5, 30]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## forward pass\n",
    "\n",
    "model = BigramModel()\n",
    "x = torch.tensor(encode_text(\"hello\"))\n",
    "logits = model.forward(x)\n",
    "vocab_size, x.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baacf804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "model = BigramModel()\n",
    "\n",
    "# this is the optimization which update the parameters etc\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "for step in range(2000):\n",
    "    x, y = get_batch(data)\n",
    "    logits = model(x)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = nn.functional.cross_entropy(logits, y)\n",
    "    # updating the params\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "788cbf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'this mo morllllorldul'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling\n",
    "\n",
    "encoded = encode_text(\"t\")\n",
    "input = torch.tensor(encoded)\n",
    "output = encoded\n",
    "\n",
    "for _ in range(20):\n",
    "    logits = model(input)\n",
    "    # converting logits into probabilities\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    # find the most likely one\n",
    "    c_index = torch.multinomial(probs, num_samples = 1)\n",
    "    # so update the input as the next time it will generate using the newly generated input\n",
    "    input = torch.tensor([c_index.item()])\n",
    "    # append it to the output list\n",
    "    output.append(c_index.item())\n",
    "\n",
    "decode_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d89249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
